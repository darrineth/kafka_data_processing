{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "from pathlib import Path\n",
    "from pyspark.sql.types import StructType, DoubleType, StringType\n",
    "from pyspark.sql.functions import col, struct, lit, to_json\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "import requests\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(\"spark.sql.streaming.schemaInference\", True)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-dryer",
   "metadata": {},
   "source": [
    "Verify that spark is running by visiting:\n",
    "    \n",
    "http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-romantic",
   "metadata": {},
   "source": [
    "Now we will download air quality data for the entire country of Poland for a single day. PM2.5 and PM10 values will be written separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://api.openaq.org/v1/measurements?country=PL&parameter={}&date_from={}&date_to={}&format=csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_in_the_past0 = datetime(2020, 12, 1)\n",
    "day_in_the_past1 = day_in_the_past0 + timedelta(days=1)\n",
    "\n",
    "day_in_the_past0 = day_in_the_past0.strftime(\"%Y-%m-%d\")\n",
    "day_in_the_past1 = day_in_the_past1.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25 = 'pm25'\n",
    "pm10 = 'pm10'\n",
    "\n",
    "parameters = [\n",
    "    pm25,\n",
    "    pm10\n",
    "]\n",
    "\n",
    "base_dir_pl = os.getcwd()+'/monitoring_data_pl_{}'\n",
    "base_path_pl = '{}/{}_{}.csv'\n",
    "\n",
    "for parameter in parameters:\n",
    "    # Create directory for the data.\n",
    "    parameter_dir = str.format(base_dir_pl, parameter)\n",
    "    dirpath = Path(parameter_dir)\n",
    "    if dirpath.exists() and dirpath.is_dir():\n",
    "        shutil.rmtree(dirpath)\n",
    "    os.makedirs(parameter_dir)\n",
    "    \n",
    "    # Download the data itself\n",
    "    url = str.format(base_url, parameter, day_in_the_past0, day_in_the_past1)\n",
    "    parameter_csv = requests.get(url)\n",
    "    with open(str.format(base_path_pl, parameter_dir, day_in_the_past0, parameter), 'bw+') as f:\n",
    "        f.write(parameter_csv.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-excellence",
   "metadata": {},
   "source": [
    "Schema specific to the data.\n",
    "\n",
    "At this point we are making some simplifications. For example ommitting the `TimestampType` in favour of `StringType`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "openAQSchema = StructType()\\\n",
    "              .add(\"location\", StringType())\\\n",
    "              .add(\"city\", StringType())\\\n",
    "              .add(\"country\", StringType())\\\n",
    "              .add(\"utc\", StringType())\\\n",
    "              .add(\"local\", StringType())\\\n",
    "              .add(\"parameter\", StringType())\\\n",
    "              .add(\"value\", DoubleType())\\\n",
    "              .add(\"unit\", StringType())\\\n",
    "              .add(\"latitude\", DoubleType())\\\n",
    "              .add(\"longitude\", DoubleType())\\\n",
    "              .add(\"attribution\", StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-clark",
   "metadata": {},
   "source": [
    "In this example we will read data from a `*.csv` source but obiously you can use any other stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "smog_stream25 = spark\\\n",
    "    .readStream\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"checkpointLocation\", \"checkpoint\")\\\n",
    "    .schema(openAQSchema)\\\n",
    "    .csv(str.format(base_dir_pl, pm25)+'/*.csv')\n",
    "\n",
    "smog_stream10 = spark\\\n",
    "    .readStream\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"checkpointLocation\", \"checkpoint\")\\\n",
    "    .schema(openAQSchema)\\\n",
    "    .csv(str.format(base_dir_pl, pm10)+'/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-portable",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "smog_stream25.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-racing",
   "metadata": {},
   "source": [
    "Now according to documentation the dataframe needs specific columns to be present:\n",
    "    \n",
    "https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#writing-data-to-kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "smog_stream25 = smog_stream25\\\n",
    "    .withColumn(\\\n",
    "               'kafka_value',\\\n",
    "               to_json(struct(*[col('location').alias('location'),\\\n",
    "                         col('city').alias('city'),\\\n",
    "                         col('country').alias('country'),\\\n",
    "                         col('utc').alias('timestamp'),\\\n",
    "                         col('parameter').alias('parameter'),\\\n",
    "                         col('value').alias('value'),\\\n",
    "                         col('unit').alias('unit'),\\\n",
    "                         col('latitude').alias('latitude'),\\\n",
    "                         col('longitude').alias('longitude'),\\\n",
    "                         col('attribution').alias('attribution')])))\\\n",
    "    .withColumn('key', lit('pm10'))\\\n",
    "    .select(col('key'), col('kafka_value'))\\\n",
    "    .withColumnRenamed('kafka_value', 'value')\n",
    "\n",
    "smog_stream10 = smog_stream10\\\n",
    "    .withColumn(\\\n",
    "               'kafka_value',\\\n",
    "               to_json(struct(*[col('location').alias('location'),\\\n",
    "                         col('city').alias('city'),\\\n",
    "                         col('country').alias('country'),\\\n",
    "                         col('utc').alias('timestamp'),\\\n",
    "                         col('parameter').alias('parameter'),\\\n",
    "                         col('value').alias('value'),\\\n",
    "                         col('unit').alias('unit'),\\\n",
    "                         col('latitude').alias('latitude'),\\\n",
    "                         col('longitude').alias('longitude'),\\\n",
    "                         col('attribution').alias('attribution')])))\\\n",
    "    .withColumn('key', lit('pm10'))\\\n",
    "    .select(col('key'), col('kafka_value'))\\\n",
    "    .withColumnRenamed('kafka_value', 'value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-serve",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smog_stream25.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-manitoba",
   "metadata": {},
   "source": [
    "Now sending data from a spark RDD to a Kafka broker is as simple as:\n",
    "\n",
    "https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#writing-data-to-kafka\n",
    "\n",
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pm25 = smog_stream25\\\n",
    "    .writeStream\\\n",
    "    .format('kafka')\\\n",
    "    .option('kafka.bootstrap.servers', 'kafka_broker:9093')\\\n",
    "    .option(\"topic\", \"pm25_topic\")\\\n",
    "    .option(\"checkpointLocation\", \"/tmp/jovyan/checkpoint\")\\\n",
    "    .trigger(processingTime='6 seconds')\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pm25.awaitTermination(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pm10 = smog_stream10\\\n",
    "    .writeStream\\\n",
    "    .format('kafka')\\\n",
    "    .option('kafka.bootstrap.servers', 'kafka_broker:9093')\\\n",
    "    .option(\"topic\", \"pm25_topic\")\\\n",
    "    .option(\"checkpointLocation\", \"/tmp/jovyan/checkpoint\")\\\n",
    "    .trigger(processingTime='6 seconds')\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pm10.awaitTermination(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-tonight",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Rafał Rak"
   },
   {
    "name": "Radosław Chrzanowski"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
