{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer, KafkaAdminClient\n",
    "from kafka.structs import TopicPartition\n",
    "from json import loads\n",
    "from time import sleep\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-capture",
   "metadata": {},
   "source": [
    "# What is Apache Kafka?\n",
    "\n",
    "Apache Kafka is a distributed data store optimized for ingesting and processing streaming data in real-time. Streaming data is data that is continuously generated by thousands of data sources, which typically send the data records in simultaneously. A streaming platform needs to handle this constant influx of data, and process the data sequentially and incrementally.\n",
    "\n",
    "Kafka provides three main functions to its users:\n",
    "\n",
    "* Publish and subscribe to streams of records\n",
    "* Effectively store streams of records in the order in which records were generated\n",
    "* Process streams of records in real time\n",
    "\n",
    "Kafka is primarily used to build real-time streaming data pipelines and applications that adapt to the data streams. It combines messaging, storage, and stream processing to allow storage and analysis of both historical and real-time data.  \n",
    "\n",
    "Source: https://aws.amazon.com/msk/what-is-kafka/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-transport",
   "metadata": {},
   "source": [
    "# How does Kafka work?\n",
    "\n",
    "Kafka combines two messaging models, queuing and publish-subscribe, to provide the key benefits of each to consumers. Queuing allows for data processing to be distributed across many consumer instances, making it highly scalable. However, traditional queues aren’t multi-subscriber. The publish-subscribe approach is multi-subscriber, but because every message goes to every subscriber it cannot be used to distribute work across multiple worker processes. Kafka uses a partitioned log model to stitch together these two solutions. A log is an ordered sequence of records, and these logs are broken up into segments, or partitions, that correspond to different subscribers. This means that there can be multiple subscribers to the same topic and each is assigned a partition to allow for higher scalability. Finally, Kafka’s model provides replayability, which allows multiple independent applications reading from data streams to work independently at their own rate.\n",
    "\n",
    "Source: https://aws.amazon.com/msk/what-is-kafka/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='./static/product-page-diagram_Kafka_Queue.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-diamond",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Image(filename='./static/product-page-diagram_Kafka_PubSub.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-visit",
   "metadata": {},
   "source": [
    "Kafka is a distributed system consisting of servers and clients that communicate via a high-performance TCP network protocol. It can be deployed on bare-metal hardware, virtual machines, and containers in on-premise as well as cloud environments.\n",
    "\n",
    "**Servers**: Kafka is run as a cluster of one or more servers that can span multiple datacenters or cloud regions. Some of these servers form the storage layer, called the brokers. Other servers run Kafka Connect to continuously import and export data as event streams to integrate Kafka with your existing systems such as relational databases as well as other Kafka clusters. To let you implement mission-critical use cases, a Kafka cluster is highly scalable and fault-tolerant: if any of its servers fails, the other servers will take over their work to ensure continuous operations without any data loss.\n",
    "\n",
    "**Clients**: They allow you to write distributed applications and microservices that read, write, and process streams of events in parallel, at scale, and in a fault-tolerant manner even in the case of network problems or machine failures. Kafka ships with some such clients included, which are augmented by dozens of clients provided by the Kafka community: clients are available for Java and Scala including the higher-level Kafka Streams library, for Go, Python, C/C++, and many other programming languages as well as REST APIs.\n",
    "\n",
    "Source: https://kafka.apache.org/intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-excitement",
   "metadata": {},
   "source": [
    "# Main Concepts and Terminology\n",
    "\n",
    "An **event** records the fact that \"something happened\" in the world or in your business. It is also called record or message in the documentation. When you read or write data to Kafka, you do this in the form of events. Conceptually, an event has a key, value, timestamp, and optional metadata headers. Here's an example event:\n",
    "\n",
    "* Event key: \"Alice\"\n",
    "* Event value: \"Made a payment of $200 to Bob\"\n",
    "* Event timestamp: \"Jun. 25, 2020 at 2:06 p.m.\"\n",
    "\n",
    "**Producers** are those client applications that publish (write) events to Kafka, and **consumers** are those that subscribe to (read and process) these events. In Kafka, producers and consumers are fully decoupled and agnostic of each other, which is a key design element to achieve the high scalability that Kafka is known for. For example, producers never need to wait for consumers. Kafka provides various guarantees such as the ability to process events exactly-once.\n",
    "\n",
    "Events are organized and durably stored in **topics**. Very simplified, a topic is similar to a folder in a filesystem, and the events are the files in that folder. An example topic name could be \"payments\". Topics in Kafka are always multi-producer and multi-subscriber: a topic can have zero, one, or many producers that write events to it, as well as zero, one, or many consumers that subscribe to these events. Events in a topic can be read as often as needed—unlike traditional messaging systems, events are not deleted after consumption. Instead, you define for how long Kafka should retain your events through a per-topic configuration setting, after which old events will be discarded. Kafka's performance is effectively constant with respect to data size, so storing data for a long time is perfectly fine.\n",
    "\n",
    "Topics are **partitioned**, meaning a topic is spread over a number of \"buckets\" located on different Kafka brokers. This distributed placement of your data is very important for scalability because it allows client applications to both read and write the data from/to many brokers at the same time. When a new event is published to a topic, it is actually appended to one of the topic's partitions. Events with the same event key (e.g., a customer or vehicle ID) are written to the same partition, and Kafka guarantees that any consumer of a given topic-partition will always read that partition's events in exactly the same order as they were written.\n",
    "\n",
    "Source: https://kafka.apache.org/intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-alberta",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Image(filename='./static/streams-and-tables-p1_p4.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-plasma",
   "metadata": {},
   "source": [
    "To make your data fault-tolerant and highly-available, every topic can be **replicated**, even across geo-regions or datacenters, so that there are always multiple brokers that have a copy of the data just in case things go wrong, you want to do maintenance on the brokers, and so on. A common production setting is a replication factor of 3, i.e., there will always be three copies of your data. This replication is performed at the level of topic-partitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-scanner",
   "metadata": {},
   "source": [
    "In the cell below we are defining a KafkaConsumer that contacts the server `kafka_broker:9093` and is subscribed to the topic `simple_topic`. Since in the producer script the message is jsonfied and encoded, here we decode it by using a lambda function in value_deserializer.\n",
    "\n",
    "* `auto_offset_reset` is a parameter that sets the policy for resetting offsets on OffsetOutOfRange errors; if you set `earliest` then it will move to the oldest available message, if `latest` is set then it will move to the most recent;\n",
    "* `enable_auto_commit` is a boolean parameter that states whether the offset will be periodically committed in the background;\n",
    "* `group_id` is the name of the consumer group to join."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-norway",
   "metadata": {},
   "source": [
    "## 0. Throughout the tutorial we will use KafkaAdminClient to monitor state of the cluster.\n",
    "\n",
    "https://kafka-python.readthedocs.io/en/master/apidoc/KafkaAdminClient.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_client = KafkaAdminClient(bootstrap_servers=['kafka_broker:9093'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-database",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "admin_client.describe_consumer_groups(group_ids=['my-group-id', 'multiple-consumers-group'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-honolulu",
   "metadata": {},
   "source": [
    "As you can see topics we created are present on the cluster but consumer groups for those topics are empty(`members=[]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-greenhouse",
   "metadata": {},
   "source": [
    "## 1. Before running cells below execute cells up to `2. Single producer - multiple consumers` in the `kafka_producers` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    "    'simple_topic',\n",
    "    bootstrap_servers=['kafka_broker:9093'],\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=True,\n",
    "    group_id='my-group-id',\n",
    "    value_deserializer=lambda x: loads(x.decode('utf-8'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-parliament",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _idx, event in enumerate(consumer):\n",
    "    event_data = event.value\n",
    "    # Do whatever you want\n",
    "    print(event_data)\n",
    "    if _idx == 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-ocean",
   "metadata": {},
   "source": [
    "The consumer group was automatically activated through the use of `for ... in consumer` syntax. We can now verify it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_client.list_consumer_groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_client.describe_consumer_groups(group_ids=['my-group-id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-sound",
   "metadata": {},
   "source": [
    "The consumer now has a partition assigned. And since we already read from the queue a non-zero offset as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topicPartition in consumer.assignment():\n",
    "    print(topicPartition)\n",
    "    print('Previous offset: ', consumer.position(topicPartition))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-penalty",
   "metadata": {},
   "source": [
    "As you can see we generated 20 values with the producer but only consumed 10 of them. Let's see what happens if we continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _idx, event in enumerate(consumer):\n",
    "    event_data = event.value\n",
    "    # Do whatever you want\n",
    "    print(event_data)\n",
    "    if _idx == 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-final",
   "metadata": {},
   "source": [
    "The consumer picked up right up where we left it.\n",
    "\n",
    "We can check and see what is the current offset for the consumer. We might as well set it to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topicPartition in consumer.assignment():\n",
    "    print(topicPartition)\n",
    "    print('Previous offset: ', consumer.position(topicPartition))\n",
    "    consumer.seek(topicPartition, 5)\n",
    "    print('New offset: ', consumer.position(topicPartition))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-sweden",
   "metadata": {},
   "source": [
    "Now that the consumer is set to a smaller offset we can process those messages we have already read. \n",
    "\n",
    "The code below will not finish execution as the consumer is waiting for new input from the queue. You might run the producer once again or simply interrupt execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-comparative",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for event in consumer:\n",
    "    event_data = event.value\n",
    "    print(event_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-developer",
   "metadata": {},
   "source": [
    "## 2. Multiple Consumers\n",
    "\n",
    "In this part we are going to see how to use Kafka with multiple consumer as a queue which basically means that every message will only be consumed once.\n",
    "\n",
    "Instead of using the `for ... in` syntax you are going to manually subscribe to a specific partition withing a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_consumer_0 = KafkaConsumer(\n",
    "    bootstrap_servers=['kafka_broker:9093'],\n",
    "    client_id = 'ensemble_consumer_0',\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=True,\n",
    "    group_id='multiple-consumers-group',\n",
    "    value_deserializer=lambda x: loads(x.decode('utf-8'))\n",
    ")\n",
    "\n",
    "ensemble_consumer_1 = KafkaConsumer(\n",
    "    bootstrap_servers=['kafka_broker:9093'],\n",
    "    client_id = 'ensemble_consumer_1',\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=True,\n",
    "    group_id='multiple-consumers-group',\n",
    "    value_deserializer=lambda x: loads(x.decode('utf-8'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-research",
   "metadata": {},
   "source": [
    "Our new consumers don't have any partition assigned so we have to do that manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ensemble_consumer_0.assignment())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ensemble_consumer_1.assignment())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-curve",
   "metadata": {},
   "source": [
    "https://kafka-python.readthedocs.io/en/master/apidoc/KafkaConsumer.html#kafka.KafkaConsumer.assign\n",
    "\n",
    "Note that:\n",
    "\n",
    "* Manual topic assignment through this method does not use the consumer’s group management functionality. As such, there will be no rebalance operation triggered when group membership or cluster and topic metadata change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_partition = TopicPartition(topic='multiple_consumers_topic', partition=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_consumer_0.assign([topic_partition])\n",
    "ensemble_consumer_1.assign([topic_partition])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-freeware",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "admin_client.describe_consumer_groups(group_ids=['multiple_consumers_group'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-bacon",
   "metadata": {},
   "source": [
    "Before executing this cell run cells under `2. Single producer - multiple consumers` in the `kafka_producers` notebook.\n",
    "\n",
    "Let's see how a full record looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-coaching",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ensemble_consumer_0.poll(timeout_ms=50, max_records=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-france",
   "metadata": {},
   "source": [
    "The important thing to note here is that as we consume events with either consumer the offset only changes for the one that processed the event. It means that multiple consumers can independently work through the partition. This is an example for the `PubSub` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topicPartition in ensemble_consumer_0.assignment():\n",
    "    print('ensemble_consumer_0 offset: ', ensemble_consumer_0.position(topicPartition))\n",
    "for topicPartition in ensemble_consumer_1.assignment():\n",
    "    print('ensemble_consumer_1 offset: ', ensemble_consumer_1.position(topicPartition))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-ground",
   "metadata": {},
   "source": [
    "Now let's try the other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-finance",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ensemble_consumer_1.poll(timeout_ms=50, max_records=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topicPartition in ensemble_consumer_0.assignment():\n",
    "    print('ensemble_consumer_0 offset: ', ensemble_consumer_0.position(topicPartition))\n",
    "\n",
    "for topicPartition in ensemble_consumer_1.assignment():\n",
    "    print('ensemble_consumer_1 offset: ', ensemble_consumer_1.position(topicPartition))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-stocks",
   "metadata": {},
   "source": [
    "The benefit of using this approach is that it's non-blocking and we can precisely control how many events we want to read.\n",
    "\n",
    "https://kafka-python.readthedocs.io/en/master/apidoc/KafkaConsumer.html#kafka.KafkaConsumer.poll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-bolivia",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _idx in range(9):\n",
    "    value0 = ensemble_consumer_0.poll(timeout_ms=50, max_records=1).get(topic_partition)[0].value\n",
    "    value1 = ensemble_consumer_1.poll(timeout_ms=50, max_records=1).get(topic_partition)[0].value\n",
    "    print('ensemble_consumer_0', value0)\n",
    "    print('ensemble_consumer_1', value1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-satisfaction",
   "metadata": {},
   "source": [
    "## 3. Reading from Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25_consumer = KafkaConsumer(\n",
    "    'pm25_topic',\n",
    "    bootstrap_servers=['kafka_broker:9093'],\n",
    "    client_id = 'pm25_consumer',\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=True,\n",
    "    group_id='pm25_consumer-group',\n",
    "    value_deserializer=lambda x: loads(x.decode('utf-8'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-modern",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in pm25_consumer:\n",
    "    print(event.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-prince",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Rafał Rak"
   },
   {
    "name": "Radosław Chrzanowski"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
